#!/usr/bin/env python3
"""
æ‰¹æ¬¡æ¨è«–å¼•æ“ (Batch Inference Engine)
ç‰ˆæœ¬: 2.1 (2025å¹´1æœˆä¿®æ­£ç‰ˆ - èªè­‰ç³»çµ±å¼·åŒ–)

åŠŸèƒ½:
1. âœ… æ”¯æ´ Google STT V2 API (Chirp 3/Telephony/2)
2. âœ… æ”¯æ´ Whisper (large-v3, turbo, medium)
3. âœ… æ”¯æ´ Gemini (2.0-flash-exp)
4. âœ… è‡ªå‹•è¼‰å…¥è©å½™è¡¨ (google_phrases.json)
5. âœ… å¾Œè™•ç†ä¿®æ­£ (text_cleaner.fix_radio_jargon)
6. âœ… æ”¯æ´ --test-case è‡ªå‹•è·¯å¾‘ç”Ÿæˆ
7. âœ… æ”¯æ´ --stt-model æŒ‡å®š STT å­æ¨¡å‹
8. âœ… å¼·åŒ–èªè­‰ç³»çµ± (è‡ªå‹•é©—è­‰æœå‹™å¸³æˆ¶é‡‘é‘°)

æ›´æ–°ç´€éŒ„ (v2.1):
- ä¿®æ­£ setup_google_credentials() å‡½æ•¸ï¼Œå¢åŠ é‡‘é‘°é©—è­‰
- è‡ªå‹•éæ¿¾é…ç½®æª”æ¡ˆï¼Œåªä½¿ç”¨æœ‰æ•ˆçš„æœå‹™å¸³æˆ¶é‡‘é‘°
- ç§»é™¤ Chirp 3 ä¸æ”¯æ´çš„ speaker diarization åƒæ•¸
- è‡ªå‹•è¨­å®š GOOGLE_CLOUD_PROJECT ç’°å¢ƒè®Šæ•¸

æ›´æ–°ç´€éŒ„ (v2.2)_2026.01.18
1. æ–°å¢äº’å‹•å¼æ“ä½œä»‹é¢
    a. é¸æ“‡è·¯å¾‘æ¨¡å¼(æ¸¬è©¦æ¡ˆä¾‹/æ‰‹å‹•è·¯å¾‘)
    b. é¸æ“‡æ¨¡å‹(Google STT/Gemini/Whisper)
    c. é¸æ“‡å­æ¨¡å‹
    d. ç¢ºèªåŸ·è¡Œ
2. æ–°å¢Geminiå­æ¨¡å‹
    a. gemini-2.0-flash-exp (æœ€æ–°ã€å¿«é€Ÿã€æˆæœ¬ä½ï¼Œé©ç”¨æ—¥å¸¸æ¸¬è©¦)
    b. gemini-1.5-pro (ç©©å®šã€æº–ç¢ºæ€§æœ€é«˜ï¼Œé‡è¦ä»»å‹™ã€ç”Ÿç”¢ç’°å¢ƒ)
    c. gemini-1.5-flash (æœ€å¿«ã€æˆæœ¬æœ€ä½ï¼Œå¤§é‡æª”æ¡ˆè™•ç†)

ä½¿ç”¨æ–¹å¼:
    # æ–¹å¼ 1: äº’å‹•å¼ä»‹é¢ï¼ˆæ¨è–¦æ–°æ‰‹ï¼‰
    python scripts/batch_inference.py
    
    # æ–¹å¼ 2: å‘½ä»¤åˆ—åƒæ•¸ï¼ˆæ¨è–¦é€²éšç”¨æˆ¶ï¼‰
    python scripts/batch_inference.py --test-case Test_02_TMRT --model gemini --gemini-model 2.0-flash-exp

"""

import os
import sys
import json
import argparse
from pathlib import Path
from datetime import datetime

# ============================================================================
# è¨­å®šè·¯å¾‘
# ============================================================================
SCRIPT_DIR = Path(__file__).parent
PROJECT_ROOT = SCRIPT_DIR.parent
sys.path.insert(0, str(PROJECT_ROOT))


# ============================================================================
# å…§å»ºèªè­‰è¨­å®š (ä¿®æ­£ç‰ˆ - å¢åŠ é‡‘é‘°é©—è­‰)
# ============================================================================
def setup_google_credentials():
    """
    è‡ªå‹•è¨­å®š Google Cloud èªè­‰ï¼ˆä¿®æ­£ç‰ˆï¼‰
    
    æ”¹é€²:
    - é©—è­‰æœå‹™å¸³æˆ¶é‡‘é‘°æ ¼å¼
    - è‡ªå‹•éæ¿¾é…ç½®æª”æ¡ˆ
    - è¨­å®šå°ˆæ¡ˆ ID
    """
    # å¦‚æœç’°å¢ƒè®Šæ•¸å·²è¨­å®šä¸”æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨
    existing_creds = os.getenv('GOOGLE_APPLICATION_CREDENTIALS')
    if existing_creds and Path(existing_creds).exists():
        # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æœå‹™å¸³æˆ¶é‡‘é‘°
        try:
            with open(existing_creds, 'r') as f:
                key_data = json.load(f)
            if key_data.get('type') == 'service_account':
                # ç’°å¢ƒè®Šæ•¸æœ‰æ•ˆï¼Œç›´æ¥ä½¿ç”¨
                return True
        except:
            # ç’°å¢ƒè®Šæ•¸æŒ‡å‘çš„é‡‘é‘°ç„¡æ•ˆï¼Œç¹¼çºŒæœå°‹
            pass
    
    # è‡ªå‹•æœå°‹é‡‘é‘°æª”æ¡ˆ
    possible_paths = [
        PROJECT_ROOT / "utils" / "google-speech-key.json",
        PROJECT_ROOT / "config" / "google-speech-key.json",
        PROJECT_ROOT / "google-speech-key.json",
    ]
    
    for key_path in possible_paths:
        if not key_path.exists():
            continue
        
        # é©—è­‰æ˜¯å¦ç‚ºæœ‰æ•ˆçš„æœå‹™å¸³æˆ¶é‡‘é‘°
        try:
            with open(key_path, 'r') as f:
                key_data = json.load(f)
            
            # å¿…é ˆæ˜¯æœå‹™å¸³æˆ¶é‡‘é‘°
            if key_data.get('type') != 'service_account':
                continue
            
            # æª¢æŸ¥å¿…è¦æ¬„ä½
            required_fields = ['project_id', 'private_key', 'client_email']
            if not all(field in key_data for field in required_fields):
                continue
            
            # è¨­å®šç’°å¢ƒè®Šæ•¸
            os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = str(key_path)
            if 'project_id' in key_data:
                os.environ['GOOGLE_CLOUD_PROJECT'] = key_data['project_id']
            
            return True
        
        except Exception:
            continue
    
    # æ‰¾ä¸åˆ°æœ‰æ•ˆçš„é‡‘é‘°
    return False


# åœ¨ import ä¹‹å‰è¨­å®šèªè­‰
setup_google_credentials()


# ============================================================================
# å°å…¥æ¨¡çµ„
# ============================================================================
try:
    from tqdm import tqdm
    TQDM_AVAILABLE = True
except ImportError:
    TQDM_AVAILABLE = False
    print("âš ï¸  tqdm ä¸å¯ç”¨ï¼Œå°‡ä½¿ç”¨ç°¡å–®é€²åº¦é¡¯ç¤º")

try:
    from utils.logger import get_logger
except ImportError:
    import logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    def get_logger(name):
        return logging.getLogger(name)

try:
    from utils.text_cleaner import fix_radio_jargon
except ImportError:
    def fix_radio_jargon(text):
        """Fallback: ä¸åšä»»ä½•è™•ç†"""
        return text


logger = get_logger(__name__)

# ============================================================================
# äº’å‹•å¼ä»‹é¢
# ============================================================================
def interactive_mode():
    """
    äº’å‹•å¼æ“ä½œå¼•å°
    
    ç•¶ä¸æä¾›å‘½ä»¤åˆ—åƒæ•¸æ™‚è‡ªå‹•å•Ÿå‹•ï¼Œå¼•å°ç”¨æˆ¶å®Œæˆè¨­å®š
    """
    print("\n" + "=" * 70)
    print("ğŸ¯ æ‰¹æ¬¡æ¨è«–å¼•æ“ - äº’å‹•å¼è¨­å®š")
    print("=" * 70)
    print()
    
    # ========================================================================
    # æ­¥é©Ÿ 1: é¸æ“‡è·¯å¾‘æ¨¡å¼
    # ========================================================================
    print("ã€æ­¥é©Ÿ 1/4ã€‘é¸æ“‡è·¯å¾‘æ¨¡å¼")
    print("-" * 70)
    print("1. ä½¿ç”¨æ¸¬è©¦æ¡ˆä¾‹ (æ¨è–¦)")
    print("   - è‡ªå‹•å¾ experiments/{æ¸¬è©¦æ¡ˆä¾‹åç¨±}/ è®€å–éŸ³æª”")
    print("   - è‡ªå‹•å„²å­˜çµæœåˆ° ASR_Evaluation/ ç›®éŒ„")
    print()
    print("2. æ‰‹å‹•æŒ‡å®šè·¯å¾‘")
    print("   - è‡ªè¡ŒæŒ‡å®šè¼¸å…¥å’Œè¼¸å‡ºç›®éŒ„")
    print()
    
    while True:
        choice = input("è«‹é¸æ“‡æ¨¡å¼ [1/2]: ").strip()
        if choice in ['1', '2']:
            break
        print("âŒ è«‹è¼¸å…¥ 1 æˆ– 2")
    
    if choice == '1':
        # æ¨¡å¼ 1: æ¸¬è©¦æ¡ˆä¾‹
        print()
        print("ğŸ“ å¯ç”¨çš„æ¸¬è©¦æ¡ˆä¾‹:")
        
        # åˆ—å‡ºå¯ç”¨çš„æ¸¬è©¦æ¡ˆä¾‹
        experiments_dir = PROJECT_ROOT / "experiments"
        if experiments_dir.exists():
            test_cases = [d.name for d in experiments_dir.iterdir() if d.is_dir()]
            if test_cases:
                for i, tc in enumerate(sorted(test_cases), 1):
                    print(f"   {i}. {tc}")
            else:
                print("   (æœªæ‰¾åˆ°æ¸¬è©¦æ¡ˆä¾‹)")
        
        print()
        test_case = input("è«‹è¼¸å…¥æ¸¬è©¦æ¡ˆä¾‹åç¨± (ä¾‹: Test_02_TMRT): ").strip()
        
        input_dir = PROJECT_ROOT / "experiments" / test_case / "source_audio"
        output_base = PROJECT_ROOT / "experiments" / test_case / "ASR_Evaluation"
        
        # æª¢æŸ¥è¼¸å…¥ç›®éŒ„
        if not input_dir.exists():
            print(f"\nâŒ éŒ¯èª¤: æ‰¾ä¸åˆ°éŸ³æª”ç›®éŒ„: {input_dir}")
            print("   è«‹ç¢ºèªæ¸¬è©¦æ¡ˆä¾‹åç¨±æ˜¯å¦æ­£ç¢º")
            sys.exit(1)
        
    else:
        # æ¨¡å¼ 2: æ‰‹å‹•è·¯å¾‘
        print()
        input_dir = Path(input("è«‹è¼¸å…¥éŸ³æª”ç›®éŒ„è·¯å¾‘: ").strip())
        output_dir = Path(input("è«‹è¼¸å…¥è¼¸å‡ºç›®éŒ„è·¯å¾‘: ").strip())
        
        if not input_dir.exists():
            print(f"\nâŒ éŒ¯èª¤: è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
            sys.exit(1)
    
    # ========================================================================
    # æ­¥é©Ÿ 2: é¸æ“‡æ¨¡å‹é¡å‹
    # ========================================================================
    print()
    print("ã€æ­¥é©Ÿ 2/4ã€‘é¸æ“‡èªéŸ³è¾¨è­˜æ¨¡å‹")
    print("-" * 70)
    print("1. Google STT (Cloud Speech-to-Text)")
    print("   - å„ªé»: ç©©å®šã€å¿«é€Ÿã€æ”¯æ´å°ˆæ¥­è©å½™")
    print("   - é©åˆ: ç”Ÿç”¢ç’°å¢ƒã€å¤§é‡éŸ³æª”")
    print()
    print("2. Google Gemini")
    print("   - å„ªé»: æœ€æ–°æŠ€è¡“ã€å¼·å¤§çš„ä¸Šä¸‹æ–‡ç†è§£")
    print("   - é©åˆ: æ¸¬è©¦ã€è¤‡é›œå°è©±")
    print()
    print("3. Whisper (OpenAI)")
    print("   - å„ªé»: æœ¬åœ°é‹è¡Œã€ç„¡ API æˆæœ¬")
    print("   - é©åˆ: é›¢ç·šç’°å¢ƒã€éš±ç§éœ€æ±‚")
    print()
    
    while True:
        model_choice = input("è«‹é¸æ“‡æ¨¡å‹ [1/2/3]: ").strip()
        if model_choice in ['1', '2', '3']:
            break
        print("âŒ è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
    
    model_map = {'1': 'google_stt', '2': 'gemini', '3': 'whisper'}
    model_type = model_map[model_choice]
    
    # ========================================================================
    # æ­¥é©Ÿ 3: é¸æ“‡å­æ¨¡å‹ï¼ˆå¦‚é©ç”¨ï¼‰
    # ========================================================================
    stt_model = "chirp_3"
    gemini_model = "gemini-2.0-flash-exp"
    
    if model_type == 'google_stt':
        print()
        print("ã€æ­¥é©Ÿ 3/4ã€‘é¸æ“‡ Google STT å­æ¨¡å‹")
        print("-" * 70)
        print("1. Chirp 3 (æ¨è–¦)")
        print("   - æœ€æ–°æ¨¡å‹ï¼Œæº–ç¢ºåº¦é«˜")
        print()
        print("2. Chirp Telephony")
        print("   - é›»è©±/ç„¡ç·šé›»å°ˆç”¨")
        print()
        print("3. Chirp 2")
        print("   - æ”¯æ´è¬›è€…è­˜åˆ¥")
        print()
        
        while True:
            stt_choice = input("è«‹é¸æ“‡å­æ¨¡å‹ [1/2/3ï¼Œç›´æ¥Enterä½¿ç”¨é è¨­]: ").strip() or '1'
            if stt_choice in ['1', '2', '3']:
                break
            print("âŒ è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
        
        stt_map = {'1': 'chirp_3', '2': 'chirp_telephony', '3': 'chirp_2'}
        stt_model = stt_map[stt_choice]
        
        if choice == '1':
            output_dir = output_base / f"google_stt_{stt_model}_output"
        
    elif model_type == 'gemini':
        print()
        print("ã€æ­¥é©Ÿ 3/4ã€‘é¸æ“‡ Gemini æ¨¡å‹")
        print("-" * 70)
        print("1. Gemini 2.0 Flash Exp (æ¨è–¦)")
        print("   - æœ€æ–°å¯¦é©—ç‰ˆæœ¬")
        print("   - é€Ÿåº¦å¿«ã€æˆæœ¬ä½")
        print()
        print("2. Gemini 1.5 Pro")
        print("   - ç©©å®šç‰ˆæœ¬")
        print("   - æº–ç¢ºåº¦é«˜ã€åŠŸèƒ½å®Œæ•´")
        print()
        print("3. Gemini 1.5 Flash")
        print("   - è¼•é‡ç‰ˆæœ¬")
        print("   - é€Ÿåº¦æœ€å¿«")
        print()
        
        while True:
            gemini_choice = input("è«‹é¸æ“‡æ¨¡å‹ [1/2/3ï¼Œç›´æ¥Enterä½¿ç”¨é è¨­]: ").strip() or '1'
            if gemini_choice in ['1', '2', '3']:
                break
            print("âŒ è«‹è¼¸å…¥ 1ã€2 æˆ– 3")
        
        gemini_map = {
            '1': 'gemini-2.0-flash-exp',
            '2': 'gemini-1.5-pro',
            '3': 'gemini-1.5-flash'
        }
        gemini_model = gemini_map[gemini_choice]
        
        if choice == '1':
            output_dir = output_base / f"gemini_{gemini_model.replace('.', '_').replace('-', '_')}_output"
    
    else:  # whisper
        if choice == '1':
            output_dir = output_base / "whisper_output"
    
    # ========================================================================
    # æ­¥é©Ÿ 4: ç¢ºèªè¨­å®š
    # ========================================================================
    print()
    print("ã€æ­¥é©Ÿ 4/4ã€‘ç¢ºèªè¨­å®š")
    print("-" * 70)
    print(f"è¼¸å…¥ç›®éŒ„: {input_dir}")
    print(f"è¼¸å‡ºç›®éŒ„: {output_dir}")
    print(f"æ¨¡å‹é¡å‹: {model_type}")
    if model_type == 'google_stt':
        print(f"STT æ¨¡å‹: {stt_model}")
    elif model_type == 'gemini':
        print(f"Gemini æ¨¡å‹: {gemini_model}")
    print()
    
    confirm = input("ç¢ºèªé–‹å§‹åŸ·è¡Œ? [Y/n]: ").strip().lower()
    if confirm and confirm not in ['y', 'yes', 'æ˜¯']:
        print("\nâŒ å·²å–æ¶ˆ")
        sys.exit(0)
    
    print()
    print("=" * 70)
    print("ğŸš€ é–‹å§‹åŸ·è¡Œæ‰¹æ¬¡æ¨è«–...")
    print("=" * 70)
    print()
    
    return {
        'input_dir': str(input_dir),
        'output_dir': str(output_dir),
        'model_type': model_type,
        'stt_model': stt_model,
        'gemini_model': gemini_model
    }


class BatchInference:
    """
    æ‰¹æ¬¡æ¨è«–å¼•æ“
    
    æ”¯æ´çš„æ¨¡å‹:
    - whisper: OpenAI Whisper (large-v3, turbo, medium)
    - google_stt: Google Cloud Speech-to-Text V2 (chirp_3, chirp_telephony, chirp_2)
    - gemini: Google Gemini (2.0-flash-exp, 1.5-pro, 1.5-flash)
    """
    
    # æ”¯æ´çš„éŸ³æª”æ ¼å¼
    SUPPORTED_EXTENSIONS = ('.wav', '.mp3', '.m4a', '.flac', '.ogg', '.aac')
    
    def __init__(
        self,
        input_dir: str,
        output_dir: str,
        model_type: str = "google_stt",
        vocabulary_file: str = None,
        stt_model: str = "chirp_3",
        stt_region: str = None,
        gemini_model: str = "gemini-2.0-flash-exp",
        language_code: str = "cmn-Hant-TW"
    ):
        """
        åˆå§‹åŒ–æ‰¹æ¬¡æ¨è«–å¼•æ“
        
        Args:
            input_dir: è¼¸å…¥éŸ³æª”ç›®éŒ„
            output_dir: è¼¸å‡ºçµæœç›®éŒ„
            model_type: æ¨¡å‹é¡å‹ (whisper, google_stt, gemini)
            vocabulary_file: è©å½™è¡¨æª”æ¡ˆè·¯å¾‘
            stt_model: Google STT å­æ¨¡å‹ (chirp_3, chirp_telephony, chirp_2)
            stt_region: Google STT å€åŸŸ
            gemini_model: Gemini æ¨¡å‹ (gemini-2.0-flash-exp, gemini-1.5-pro, gemini-1.5-flash)
            language_code: èªè¨€ä»£ç¢¼
        """
        self.input_dir = Path(input_dir)
        self.output_dir = Path(output_dir)
        self.model_type = model_type.lower()
        self.vocabulary_file = vocabulary_file
        self.stt_model = stt_model
        self.stt_region = stt_region
        self.gemini_model = gemini_model
        self.language_code = language_code
        
        # å»ºç«‹è¼¸å‡ºç›®éŒ„
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¼‰å…¥è©å½™è¡¨
        self.phrases = self._load_vocabulary() if vocabulary_file else None
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = self._init_model()
        
        # è¨˜éŒ„é…ç½®
        logger.info("=" * 60)
        logger.info("æ‰¹æ¬¡æ¨è«–å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æ¨¡å‹é¡å‹: {model_type}")
        if model_type == "google_stt":
            logger.info(f"STT æ¨¡å‹: {stt_model}")
            logger.info(f"STT å€åŸŸ: {stt_region or 'è‡ªå‹•'}")
        elif model_type == "gemini":
            logger.info(f"Gemini æ¨¡å‹: {gemini_model}")
        logger.info(f"è¼¸å…¥ç›®éŒ„: {self.input_dir}")
        logger.info(f"è¼¸å‡ºç›®éŒ„: {self.output_dir}")
        if self.phrases:
            logger.info(f"è©å½™è¡¨: {self.phrases.get('total_terms', len(self.phrases.get('phrases', [])))} å€‹è©å½™")
        logger.info("=" * 60)
    
    def _load_vocabulary(self) -> dict:
        """è¼‰å…¥è©å½™è¡¨"""
        if not self.vocabulary_file:
            return None
        
        vocab_path = Path(self.vocabulary_file)
        if not vocab_path.exists():
            logger.warning(f"æ‰¾ä¸åˆ°è©å½™è¡¨æª”æ¡ˆ: {self.vocabulary_file}")
            return None
        
        try:
            with open(vocab_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            total = data.get('total_terms', len(data.get('phrases', [])))
            logger.info(f"âœ… è¼‰å…¥è©å½™è¡¨: {total} å€‹è©å½™")
            return data
        
        except Exception as e:
            logger.error(f"âŒ è¼‰å…¥è©å½™è¡¨å¤±æ•—: {e}")
            return None
    
    def _init_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == "whisper":
            return self._init_whisper()
        elif self.model_type == "google_stt":
            return self._init_google_stt()
        elif self.model_type == "gemini":
            return self._init_gemini()
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}")
    
    def _init_whisper(self):
        """åˆå§‹åŒ– Whisper æ¨¡å‹"""
        try:
            # å˜—è©¦ä½¿ç”¨æ–°ç‰ˆæ¨¡çµ„
            from scripts.models.model_whisper import WhisperModel
            logger.info("ä½¿ç”¨æ–°ç‰ˆ Whisper æ¨¡çµ„")
            return WhisperModel(model_size="large-v3")
        except ImportError:
            try:
                # å˜—è©¦ç›´æ¥ä½¿ç”¨ whisper
                import whisper
                logger.info("ä½¿ç”¨åŸç”Ÿ whisper æ¨¡çµ„")
                model = whisper.load_model("large-v3")
                return {"model": model, "type": "native"}
            except ImportError:
                raise ImportError("æ‰¾ä¸åˆ° Whisper æ¨¡çµ„ï¼Œè«‹å®‰è£: pip install openai-whisper")
    
    def _init_google_stt(self):
        """åˆå§‹åŒ– Google STT æ¨¡å‹"""
        # ç¢ºä¿èªè­‰å·²è¨­å®š
        if not setup_google_credentials():
            logger.warning("âš ï¸  æœªè¨­å®š Google Cloud èªè­‰")
        
        try:
            # å˜—è©¦ä½¿ç”¨æ–°ç‰ˆæ¨¡çµ„
            from scripts.models.model_google_stt import GoogleSTTModel
            
            logger.info(f"åˆå§‹åŒ– Google STT: {self.stt_model}")
            
            return GoogleSTTModel(
                project_id=os.getenv('GOOGLE_CLOUD_PROJECT', 'dazzling-seat-315406'),
                location=self.stt_region,
                model=self.stt_model,
                language_code=self.language_code,
                auto_config=True
            )
        
        except ImportError as e:
            logger.error(f"æ‰¾ä¸åˆ° Google STT æ¨¡çµ„: {e}")
            raise
    
    def _init_gemini(self):
        """åˆå§‹åŒ– Gemini æ¨¡å‹"""
        try:
            from scripts.models.model_gemini import GeminiModel
            
            logger.info(f"åˆå§‹åŒ– Gemini: {self.gemini_model}")
            
            return GeminiModel(
                model=self.gemini_model,
                temperature=0.0
            )
        
        except ImportError as e:
            logger.error(f"æ‰¾ä¸åˆ° Gemini æ¨¡çµ„: {e}")
            raise
    
    def transcribe_file(self, audio_file: Path) -> dict:
        """
        è¾¨è­˜å–®ä¸€éŸ³æª”
        
        Args:
            audio_file: éŸ³æª”è·¯å¾‘
        
        Returns:
            è¾¨è­˜çµæœå­—å…¸
        """
        if self.model_type == "whisper":
            return self._transcribe_whisper(audio_file)
        elif self.model_type == "google_stt":
            return self._transcribe_google_stt(audio_file)
        elif self.model_type == "gemini":
            return self._transcribe_gemini(audio_file)
        else:
            raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹é¡å‹: {self.model_type}")
    
    def _transcribe_whisper(self, audio_file: Path) -> dict:
        """ä½¿ç”¨ Whisper è¾¨è­˜"""
        if hasattr(self.model, 'transcribe_file'):
            # æ–°ç‰ˆæ¨¡çµ„
            result = self.model.transcribe_file(str(audio_file))
        elif isinstance(self.model, dict) and self.model.get('type') == 'native':
            # åŸç”Ÿ whisper
            raw_result = self.model['model'].transcribe(
                str(audio_file),
                language="zh",
                initial_prompt="é€™æ˜¯å°ç£æ·é‹ç„¡ç·šé›»é€šè¨ŠéŒ„éŸ³ã€‚"
            )
            result = {
                'transcript': raw_result.get('text', ''),
                'transcript_raw': raw_result.get('text', '')
            }
        else:
            raise RuntimeError("Whisper æ¨¡å‹åˆå§‹åŒ–ç•°å¸¸")
        
        return result
    
    def _transcribe_google_stt(self, audio_file: Path) -> dict:
        """ä½¿ç”¨ Google STT è¾¨è­˜"""
        phrases_list = None
        if self.phrases:
            phrases_list = self.phrases.get('phrases', [])
        
        return self.model.transcribe_file(
            str(audio_file),
            phrases=phrases_list,
            enable_word_time_offsets=True,         # å•Ÿç”¨æ™‚é–“æˆ³
            enable_automatic_punctuation=True      # å•Ÿç”¨è‡ªå‹•æ–·å¥
            # æ³¨æ„: Chirp 3 ä¸æ”¯æ´ speaker diarization åƒæ•¸
            # å¦‚éœ€è¬›è€…è­˜åˆ¥ï¼Œè«‹ä½¿ç”¨ chirp_2 æˆ– latest_long æ¨¡å‹
        )
    
    def _transcribe_gemini(self, audio_file: Path) -> dict:
        """ä½¿ç”¨ Gemini è¾¨è­˜"""
        context = "é€™æ˜¯å°ä¸­æ·é‹ç„¡ç·šé›»é€šè¨ŠéŒ„éŸ³ã€‚"
        
        if self.phrases:
            top_terms = [p.get('value', p) if isinstance(p, dict) else p 
                        for p in self.phrases.get('phrases', [])[:30]]
            if top_terms:
                context += f"\nå¸¸è¦‹è¡“èª: {', '.join(top_terms)}"
        
        return self.model.transcribe_file(
            str(audio_file),
            context=context
        )
    
    def run(self) -> dict:
        """
        åŸ·è¡Œæ‰¹æ¬¡æ¨è«–
        
        Returns:
            æ‰€æœ‰æª”æ¡ˆçš„è¾¨è­˜çµæœ
        """
        # æƒæéŸ³æª”
        audio_files = []
        for ext in self.SUPPORTED_EXTENSIONS:
            audio_files.extend(self.input_dir.glob(f"*{ext}"))
            audio_files.extend(self.input_dir.glob(f"*{ext.upper()}"))
        
        audio_files = sorted(set(audio_files))
        
        if not audio_files:
            logger.warning(f"âŒ æ‰¾ä¸åˆ°ä»»ä½•éŸ³æª”åœ¨: {self.input_dir}")
            return {}
        
        logger.info(f"æ‰¾åˆ° {len(audio_files)} å€‹éŸ³æª”ï¼Œé–‹å§‹è™•ç†...")
        
        results = {}
        success_count = 0
        error_count = 0
        
        # å»ºç«‹é€²åº¦è¿­ä»£å™¨
        if TQDM_AVAILABLE:
            iterator = tqdm(audio_files, desc=f"è™•ç† {self.model_type}")
        else:
            iterator = audio_files
        
        # è™•ç†æ¯å€‹æª”æ¡ˆ
        for audio_file in iterator:
            chunk_id = audio_file.stem
            
            if not TQDM_AVAILABLE:
                logger.info(f"è™•ç†: {audio_file.name}")
            
            try:
                # A. è¾¨è­˜
                result = self.transcribe_file(audio_file)
                
                # B. å¾Œè™•ç†ä¿®æ­£
                if 'transcript' in result:
                    result['transcript'] = fix_radio_jargon(result['transcript'])
                
                # C. å„²å­˜æ–‡å­—æª”
                txt_file = self.output_dir / f"{chunk_id}.txt"
                with open(txt_file, 'w', encoding='utf-8') as f:
                    f.write(result.get('transcript', ''))
                
                # D. è¨˜éŒ„çµæœ
                results[chunk_id] = result
                success_count += 1
                
            except Exception as e:
                logger.error(f"âŒ è™•ç†å¤±æ•— ({audio_file.name}): {e}")
                results[chunk_id] = {
                    'transcript': '',
                    'transcript_raw': '',
                    'error': str(e)
                }
                error_count += 1
        
        # å„²å­˜å®Œæ•´çµæœ JSON
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        json_file = self.output_dir / f"{self.model_type}_results_{timestamp}.json"
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump({
                'metadata': {
                    'model_type': self.model_type,
                    'stt_model': self.stt_model if self.model_type == 'google_stt' else None,
                    'timestamp': timestamp,
                    'total_files': len(results),
                    'success_count': success_count,
                    'error_count': error_count
                },
                'results': results
            }, f, ensure_ascii=False, indent=2)
        
        # è¼¸å‡ºæ‘˜è¦
        logger.info("=" * 60)
        logger.info("æ‰¹æ¬¡æ¨è«–å®Œæˆ")
        logger.info("=" * 60)
        logger.info(f"æˆåŠŸ: {success_count}/{len(results)}")
        logger.info(f"å¤±æ•—: {error_count}/{len(results)}")
        logger.info(f"çµæœå·²å„²å­˜: {self.output_dir}")
        logger.info(f"è©³ç´° JSON: {json_file}")
        logger.info("=" * 60)
        
        return results


def main():
    """å‘½ä»¤åˆ—ä»‹é¢"""
    # æª¢æŸ¥æ˜¯å¦æœ‰å‘½ä»¤åˆ—åƒæ•¸
    if len(sys.argv) == 1:
        # ç„¡åƒæ•¸ï¼Œå•Ÿå‹•äº’å‹•æ¨¡å¼
        config = interactive_mode()
        
        # è‡ªå‹•å°‹æ‰¾è©å½™è¡¨
        vocabulary_file = None
        possible_vocab_paths = [
            PROJECT_ROOT / "vocabulary" / "google_phrases.json",
            PROJECT_ROOT / "config" / "google_phrases.json",
        ]
        for vocab_path in possible_vocab_paths:
            if vocab_path.exists():
                vocabulary_file = str(vocab_path)
                logger.info(f"è‡ªå‹•è¼‰å…¥è©å½™è¡¨: {vocab_path}")
                break
        
        # å»ºç«‹ä¸¦åŸ·è¡Œæ¨è«–å¼•æ“
        engine = BatchInference(
            input_dir=config['input_dir'],
            output_dir=config['output_dir'],
            model_type=config['model_type'],
            vocabulary_file=vocabulary_file,
            stt_model=config['stt_model'],
            gemini_model=config['gemini_model'],
            language_code="cmn-Hant-TW"
        )
        
        results = engine.run()
        print(f"\nâœ¨ è™•ç†å®Œæˆï¼å…± {len(results)} å€‹æª”æ¡ˆ")
        return
    
    # æœ‰åƒæ•¸ï¼Œä½¿ç”¨æ¨™æº– argparse
    parser = argparse.ArgumentParser(
        description="æ‰¹æ¬¡æ¨è«–å¼•æ“ - æ”¯æ´ Whisper / Google STT / Gemini",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¯„ä¾‹:
  # äº’å‹•å¼ä»‹é¢ï¼ˆæ¨è–¦æ–°æ‰‹ï¼‰
  python scripts/batch_inference.py

  # ä½¿ç”¨ Chirp 3
  python scripts/batch_inference.py --test-case Test_02_TMRT --model google_stt --stt-model chirp_3

  # ä½¿ç”¨ Gemini 2.0
  python scripts/batch_inference.py --test-case Test_02_TMRT --model gemini --gemini-model gemini-2.0-flash-exp

  # ä½¿ç”¨ Gemini 1.5 Pro
  python scripts/batch_inference.py --test-case Test_02_TMRT --model gemini --gemini-model gemini-1.5-pro

  # ä½¿ç”¨ Whisper
  python scripts/batch_inference.py --test-case Test_02_TMRT --model whisper
        """
    )
    
    # æ¨¡å‹é¸æ“‡
    parser.add_argument(
        "--model",
        choices=["whisper", "google_stt", "gemini"],
        default="google_stt",
        help="ä¸»è¦æ¨¡å‹é¡å‹"
    )
    
    # Google STT å°ˆç”¨åƒæ•¸
    parser.add_argument(
        "--stt-model",
        choices=["chirp_3", "chirp_telephony", "chirp_2", "chirp", "latest_long", "latest_short", "telephony"],
        default="chirp_3",
        help="Google STT å­æ¨¡å‹ (é è¨­: chirp_3)"
    )
    
    parser.add_argument(
        "--stt-region",
        choices=["us", "eu", "us-central1", "asia-southeast1", "asia-northeast1", "europe-west4"],
        default=None,
        help="Google STT å€åŸŸ (é è¨­: è‡ªå‹•é¸æ“‡)"
    )

    # Gemini å°ˆç”¨åƒæ•¸
    parser.add_argument(
        "--gemini-model",
        choices=["gemini-2.0-flash-exp", "gemini-1.5-pro", "gemini-1.5-flash"],
        default="gemini-2.0-flash-exp",
        help="Gemini æ¨¡å‹ (é è¨­: gemini-2.0-flash-exp)"
    )
    
    # è·¯å¾‘è¨­å®š
    parser.add_argument(
        "--test-case",
        help="æ¸¬è©¦æ¡ˆåç¨± (ç”¨æ–¼è‡ªå‹•è¨­å®šè·¯å¾‘ï¼Œå¦‚ Test_02_TMRT)"
    )
    
    parser.add_argument(
        "--input-dir",
        help="è¼¸å…¥éŸ³æª”ç›®éŒ„ (èˆ‡ --test-case äºŒæ“‡ä¸€)"
    )
    
    parser.add_argument(
        "--output-dir",
        help="è¼¸å‡ºçµæœç›®éŒ„ (èˆ‡ --test-case äºŒæ“‡ä¸€)"
    )
    
    # å…¶ä»–åƒæ•¸
    parser.add_argument(
        "--vocabulary",
        help="è©å½™è¡¨æª”æ¡ˆ (google_phrases.json)"
    )
    
    parser.add_argument(
        "--language",
        default="cmn-Hant-TW",
        help="èªè¨€ä»£ç¢¼ (é è¨­: cmn-Hant-TW ç¹é«”ä¸­æ–‡)"
    )
    
    args = parser.parse_args()
    
    # ========================================================================
    # è·¯å¾‘æ±ºå®šé‚è¼¯
    # ========================================================================
    if args.test_case:
        # æ¨¡å¼ 1: ä½¿ç”¨ --test-case è‡ªå‹•ç”Ÿæˆè·¯å¾‘
        input_dir = PROJECT_ROOT / "experiments" / args.test_case / "source_audio"
        output_dir = PROJECT_ROOT / "experiments" / args.test_case / "ASR_Evaluation" / f"{args.model}_output"
        
        logger.info(f"ä½¿ç”¨æ¸¬è©¦æ¡ˆä¾‹æ¨¡å¼: {args.test_case}")
        
    elif args.input_dir and args.output_dir:
        # æ¨¡å¼ 2: æ‰‹å‹•æŒ‡å®šè·¯å¾‘
        input_dir = Path(args.input_dir)
        output_dir = Path(args.output_dir)
        
        logger.info("ä½¿ç”¨æ‰‹å‹•è·¯å¾‘æ¨¡å¼")
        
    else:
        print("âŒ éŒ¯èª¤: è«‹æä¾›ä»¥ä¸‹å…¶ä¸­ä¸€çµ„åƒæ•¸ï¼š")
        print("  æ¨¡å¼ 1: --test-case TEST_NAME")
        print("  æ¨¡å¼ 2: --input-dir INPUT_PATH --output-dir OUTPUT_PATH")
        print("\næˆ–ç›´æ¥åŸ·è¡Œä¸å¸¶åƒæ•¸é€²å…¥äº’å‹•æ¨¡å¼ï¼š")
        print("  python scripts/batch_inference.py")
        return
    
    # æª¢æŸ¥è¼¸å…¥ç›®éŒ„
    if not input_dir.exists():
        print(f"âŒ éŒ¯èª¤: è¼¸å…¥ç›®éŒ„ä¸å­˜åœ¨: {input_dir}")
        return
    
    # è‡ªå‹•å°‹æ‰¾è©å½™è¡¨
    if not args.vocabulary:
        possible_vocab_paths = [
            PROJECT_ROOT / "vocabulary" / "google_phrases.json",
            PROJECT_ROOT / "config" / "google_phrases.json",
        ]
        for vocab_path in possible_vocab_paths:
            if vocab_path.exists():
                args.vocabulary = str(vocab_path)
                logger.info(f"è‡ªå‹•è¼‰å…¥è©å½™è¡¨: {vocab_path}")
                break
    
    # ========================================================================
    # å»ºç«‹ä¸¦åŸ·è¡Œæ¨è«–å¼•æ“
    # ========================================================================
    engine = BatchInference(
        input_dir=str(input_dir),
        output_dir=str(output_dir),
        model_type=args.model,
        vocabulary_file=args.vocabulary,
        stt_model=args.stt_model,
        stt_region=args.stt_region,
        gemini_model=args.gemini_model,
        language_code=args.language
    )
    
    results = engine.run()
    
    print(f"\nâœ¨ è™•ç†å®Œæˆï¼å…± {len(results)} å€‹æª”æ¡ˆ")


if __name__ == "__main__":
    main()