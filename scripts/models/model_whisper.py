"""
Whisper èªéŸ³è¾¨è­˜æ¨¡çµ„ï¼ˆå‹•æ…‹è©å½™è¡¨ç‰ˆæœ¬ï¼‰
æ”¯æ´å¾ vocabulary/google_phrases.json å‹•æ…‹ç”¢ç”Ÿ initial_prompt

æª”æ¡ˆä½ç½®: aiSpeech/scripts/model_whisper.py

ä½¿ç”¨å‰æº–å‚™:
1. å®‰è£å¥—ä»¶: pip install openai-whisper torch
2. ç¢ºä¿ vocabulary/google_phrases.json å·²ç”¢ç”Ÿ
3. (é¸ç”¨) å¦‚æœæœ‰ NVIDIA GPU: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118

ä½¿ç”¨æ–¹å¼:
    from model_whisper import transcribe_with_whisper
    text = transcribe_with_whisper("audio.wav", model_size="large-v3")
"""

import json
import whisper
import torch
from pathlib import Path


# ==================== å…¨åŸŸè®Šæ•¸ ====================
# ç”¨ä¾†æš«å­˜è¼‰å…¥å¥½çš„æ¨¡å‹ï¼Œé¿å…é‡è¤‡è¼‰å…¥ï¼ˆå–®ä¾‹æ¨¡å¼ï¼‰
_loaded_model = None
_current_model_size = None

# æš«å­˜å·²ç”¢ç”Ÿçš„ promptï¼Œé¿å…é‡è¤‡è®€å–æª”æ¡ˆ
_cached_prompt = None
_prompt_loaded = False


# ==================== è©å½™è¡¨è™•ç† ====================

def load_vocabulary_for_prompt():
    """
    å¾ vocabulary/google_phrases.json å‹•æ…‹ç”¢ç”Ÿ Whisper çš„ initial_prompt
    
    ç”¨é€”1: è¾¨è­˜å‰å„ªåŒ–
    - å°‡è©å½™è¡¨ä¸­æ¬Šé‡æœ€é«˜çš„è¡“èªçµ„æˆ prompt
    - æç¤º Whisper æ¨¡å‹æ³¨æ„é€™äº›å°ˆæ¥­ç”¨èª
    
    Returns:
        str: æ ¼å¼åŒ–çš„ prompt æ–‡å­—
    """
    global _cached_prompt, _prompt_loaded
    
    # å¦‚æœå·²ç¶“è¼‰å…¥éï¼Œç›´æ¥è¿”å›å¿«å–
    if _prompt_loaded:
        return _cached_prompt
    
    # å–å¾—è©å½™è¡¨è·¯å¾‘
    project_root = Path(__file__).parent.parent
    phrases_path = project_root / 'vocabulary' / 'google_phrases.json'
    
    # å¦‚æœæª”æ¡ˆä¸å­˜åœ¨ï¼Œä½¿ç”¨é è¨­ prompt
    if not phrases_path.exists():
        print("âš ï¸  è­¦å‘Š: æ‰¾ä¸åˆ° google_phrases.jsonï¼Œä½¿ç”¨é è¨­ prompt")
        print(f"   è«‹å…ˆåŸ·è¡Œ: python utils/vocabulary_generator.py")
        _cached_prompt = "é€™æ˜¯ä¸€æ®µå°ç£æ·é‹ç„¡ç·šé›»é€šè¨Šã€‚"
        _prompt_loaded = True
        return _cached_prompt
    
    try:
        # è®€å–è©å½™è¡¨
        with open(phrases_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        phrases = data.get('phrases', [])
        
        if not phrases:
            print("âš ï¸  è­¦å‘Š: è©å½™è¡¨ç‚ºç©ºï¼Œä½¿ç”¨é è¨­ prompt")
            _cached_prompt = "é€™æ˜¯ä¸€æ®µå°ç£æ·é‹ç„¡ç·šé›»é€šè¨Šã€‚"
            _prompt_loaded = True
            return _cached_prompt
        
        # ä¾ç…§ boost å€¼æ’åºï¼Œå–å‰ 30 å€‹æœ€é‡è¦çš„è¡“èª
        top_terms = sorted(
            phrases, 
            key=lambda x: x.get('boost', 0), 
            reverse=True
        )[:30]
        
        # æå–è¡“èªæ–‡å­—
        terms_list = [term['value'] for term in top_terms]
        
        # çµ„åˆæˆè‡ªç„¶èªè¨€çš„ prompt
        terms_str = "ã€".join(terms_list)
        
        prompt = (
            f"é€™æ˜¯ä¸€æ®µå°ç£æ·é‹ç„¡ç·šé›»é€šè¨Šã€‚"
            f"å°è©±ä¸­åŒ…å«ä»¥ä¸‹å°ˆæ¥­è¡“èªï¼š{terms_str}ã€‚"
            f"è«‹æº–ç¢ºè¾¨è­˜é€™äº›è¡“èªï¼Œä¿æŒåŸæ–‡ä¸è¦ç¿»è­¯æˆ–è½‰æ›ã€‚"
        )
        
        _cached_prompt = prompt
        print(f"âœ… å·²è¼‰å…¥è©å½™è¡¨: {len(terms_list)} å€‹é—œéµè¡“èª")
        
    except Exception as e:
        print(f"âš ï¸  è®€å–è©å½™è¡¨å¤±æ•—: {e}")
        print("   ä½¿ç”¨é è¨­ prompt")
        _cached_prompt = "é€™æ˜¯ä¸€æ®µå°ç£æ·é‹ç„¡ç·šé›»é€šè¨Šã€‚"
    
    _prompt_loaded = True
    return _cached_prompt


# ==================== æ¨¡å‹ç®¡ç† ====================

def load_model_once(model_size="large-v3"):
    """
    ç¢ºä¿æ¨¡å‹åªè¢«è¼‰å…¥ä¸€æ¬¡çš„å–®ä¾‹æ¨¡å¼ï¼ˆSingleton Patternï¼‰
    ä¸¦ä¿ç•™åŸæœ¬çš„ M2 GPU åŠ é€Ÿåˆ¤æ–·é‚è¼¯
    
    Args:
        model_size (str): æ¨¡å‹å¤§å°
            - "turbo": æœ€å¿«ï¼Œæº–ç¢ºåº¦ç•¥ä½ï¼ˆé©åˆæ¸¬è©¦ï¼‰
            - "medium": ä¸­ç­‰é€Ÿåº¦èˆ‡æº–ç¢ºåº¦
            - "large-v3": æœ€æº–ç¢ºï¼ˆæ¨è–¦ç”¨æ–¼æ­£å¼è¾¨è­˜ï¼‰
    
    Returns:
        whisper.Whisper: è¼‰å…¥çš„ Whisper æ¨¡å‹
    """
    global _loaded_model, _current_model_size
    
    # å¦‚æœæ¨¡å‹å·²ç¶“è¼‰å…¥ä¸”å¤§å°ä¸€æ¨£ï¼Œç›´æ¥å›å‚³
    if _loaded_model is not None and _current_model_size == model_size:
        return _loaded_model
    
    print(f"ğŸ”„ æ­£åœ¨è¼‰å…¥ Whisper æ¨¡å‹ ({model_size})...")
    
    # åµæ¸¬å¯ç”¨çš„åŠ é€Ÿè£ç½®
    # å„ªå…ˆé †åº: CUDA (NVIDIA GPU) > MPS (Apple Silicon) > CPU
    if torch.cuda.is_available():
        device = "cuda"
    elif torch.backends.mps.is_available():
        device = "mps"
    else:
        device = "cpu"
    
    try:
        _loaded_model = whisper.load_model(model_size, device=device)
        print(f"âœ… Whisper æ¨¡å‹ ({model_size}) è¼‰å…¥å®Œæˆ")
        print(f"   ä½¿ç”¨è£ç½®: {device.upper()}")
    except Exception as e:
        print(f"âš ï¸  {device.upper()} å•Ÿç”¨å¤±æ•—ï¼Œåˆ‡æ›å› CPU æ¨¡å¼: {e}")
        _loaded_model = whisper.load_model(model_size, device="cpu")
        print(f"âœ… Whisper æ¨¡å‹ ({model_size}) è¼‰å…¥å®Œæˆ")
        print(f"   ä½¿ç”¨è£ç½®: CPU")
    
    _current_model_size = model_size
    return _loaded_model


# ==================== è¾¨è­˜åŠŸèƒ½ ====================

def transcribe_with_whisper(
    audio_path, 
    model_size="large-v3",
    use_vocabulary=True,
    language="zh"
):
    """
    ä½¿ç”¨ Whisper è¾¨è­˜å–®ä¸€éŸ³æª”
    
    Args:
        audio_path (str): éŸ³æª”è·¯å¾‘
        model_size (str): æ¨¡å‹å¤§å° (turbo/medium/large-v3)
        use_vocabulary (bool): æ˜¯å¦ä½¿ç”¨è©å½™è¡¨ç”¢ç”Ÿ promptï¼ˆç”¨é€”1ï¼‰
        language (str): èªè¨€ä»£ç¢¼
            - "zh": ä¸­æ–‡ï¼ˆè‡ªå‹•åµæ¸¬ç°¡ç¹ï¼‰
            - "zh-TW": ç¹é«”ä¸­æ–‡
            - None: è‡ªå‹•åµæ¸¬èªè¨€
    
    Returns:
        str: è¾¨è­˜æ–‡å­—
    """
    # 1. å–å¾—æ¨¡å‹å¯¦é«”
    model = load_model_once(model_size)
    
    # 2. ç”¢ç”Ÿ promptï¼ˆå¦‚æœå•Ÿç”¨è©å½™è¡¨ï¼‰
    if use_vocabulary:
        prompt_text = load_vocabulary_for_prompt()
    else:
        prompt_text = "é€™æ˜¯ä¸€æ®µå°ç£æ·é‹ç„¡ç·šé›»é€šè¨Šã€‚"
    
    # 3. åŸ·è¡Œè¾¨è­˜
    # initial_prompt æ˜¯ Whisper çš„é—œéµåƒæ•¸ï¼Œå¯ä»¥å¼•å°æ¨¡å‹è¾¨è­˜æ–¹å‘
    result = model.transcribe(
        audio_path,
        language=language,
        initial_prompt=prompt_text,
        verbose=False  # é—œé–‰é€²åº¦é¡¯ç¤ºï¼ˆæ‰¹æ¬¡è™•ç†æ™‚è¼ƒä¹¾æ·¨ï¼‰
    )
    
    return result['text'].strip()


def batch_transcribe(
    audio_folder, 
    output_folder, 
    model_size="large-v3",
    use_vocabulary=True
):
    """
    æ‰¹æ¬¡è¾¨è­˜è³‡æ–™å¤¾ä¸­çš„æ‰€æœ‰éŸ³æª”
    
    Args:
        audio_folder (str): éŸ³æª”è³‡æ–™å¤¾è·¯å¾‘
        output_folder (str): è¼¸å‡ºæ–‡å­—æª”è³‡æ–™å¤¾
        model_size (str): ä½¿ç”¨çš„æ¨¡å‹
        use_vocabulary (bool): æ˜¯å¦ä½¿ç”¨è©å½™è¡¨
    """
    from pathlib import Path
    
    audio_folder = Path(audio_folder)
    output_folder = Path(output_folder)
    output_folder.mkdir(parents=True, exist_ok=True)
    
    # æ”¯æ´çš„éŸ³æª”æ ¼å¼
    audio_files = []
    for ext in ['.wav', '.mp3', '.m4a', '.flac', '.ogg']:
        audio_files.extend(audio_folder.glob(f'*{ext}'))
    
    audio_files = sorted(audio_files)
    
    if not audio_files:
        print(f"âŒ åœ¨ {audio_folder} ä¸­æ‰¾ä¸åˆ°éŸ³æª”")
        return
    
    print("\n" + "="*60)
    print(f"ğŸ“‚ é–‹å§‹æ‰¹æ¬¡è™•ç†: {len(audio_files)} å€‹æª”æ¡ˆ")
    print(f"   æ¨¡å‹: Whisper {model_size}")
    print(f"   è©å½™è¡¨: {'å•Ÿç”¨' if use_vocabulary else 'åœç”¨'}")
    print(f"   è¼¸å‡º: {output_folder}")
    print("="*60 + "\n")
    
    for i, audio_path in enumerate(audio_files, 1):
        print(f"â–¶ï¸  [{i}/{len(audio_files)}] {audio_path.name}")
        
        output_path = output_folder / f"{audio_path.stem}.txt"
        
        # å¦‚æœå·²ç¶“å­˜åœ¨ï¼Œè·³é
        if output_path.exists():
            print(f"   â­ï¸  å·²å­˜åœ¨ï¼Œè·³é")
            continue
        
        try:
            text = transcribe_with_whisper(
                str(audio_path),
                model_size=model_size,
                use_vocabulary=use_vocabulary
            )
            
            # å­˜æª”
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(text)
            
            print(f"   âœ… å®Œæˆ")
            
        except Exception as e:
            print(f"   âŒ éŒ¯èª¤: {e}")


# ==================== æ¸¬è©¦ç¨‹å¼ ====================

if __name__ == "__main__":
    """
    æ¸¬è©¦ç”¨ä¸»ç¨‹å¼
    ä½¿ç”¨æ–¹å¼: python scripts/model_whisper.py
    """
    import sys
    
    print("="*60)
    print("Whisper èªéŸ³è¾¨è­˜æ¸¬è©¦ï¼ˆå‹•æ…‹è©å½™è¡¨ç‰ˆæœ¬ï¼‰")
    print("="*60)
    
    # æ¸¬è©¦è©å½™è¡¨è¼‰å…¥
    print("\nğŸ“‹ æ¸¬è©¦è©å½™è¡¨è¼‰å…¥...")
    prompt = load_vocabulary_for_prompt()
    print(f"\nç”¢ç”Ÿçš„ Prompt (å‰ 100 å­—):")
    print("-" * 60)
    print(prompt[:100] + "...")
    print("-" * 60)
    
    # æ¸¬è©¦å–®ä¸€æª”æ¡ˆè¾¨è­˜
    test_file = "experiments/Test_01_TMRT/dataset_chunks/chunk_001.wav"
    
    if Path(test_file).exists():
        print(f"\nğŸ¤ æ¸¬è©¦æª”æ¡ˆ: {test_file}")
        print("   ä½¿ç”¨æ¨¡å‹: turbo (æ¸¬è©¦ç”¨)")
        
        try:
            result = transcribe_with_whisper(
                test_file,
                model_size="turbo",  # æ¸¬è©¦æ™‚ç”¨è¼ƒå¿«çš„æ¨¡å‹
                use_vocabulary=True
            )
            
            print("\nè¾¨è­˜çµæœ:")
            print("-" * 60)
            print(result)
            print("-" * 60)
            
        except Exception as e:
            print(f"\nâŒ æ¸¬è©¦å¤±æ•—: {e}")
            import traceback
            traceback.print_exc()
    else:
        print(f"\nâš ï¸  æ‰¾ä¸åˆ°æ¸¬è©¦æª”æ¡ˆ: {test_file}")
        print("   è«‹ç¢ºèªæª”æ¡ˆè·¯å¾‘æˆ–ä¿®æ”¹ test_file è®Šæ•¸")
        
        # æç¤ºæ‰¹æ¬¡è™•ç†ç”¨æ³•
        print("\nğŸ’¡ æ‰¹æ¬¡è™•ç†ç”¨æ³•:")
        print("   from model_whisper import batch_transcribe")
        print("   batch_transcribe(")
        print("       'experiments/Test_01_TMRT/dataset_chunks',")
        print("       'experiments/Test_01_TMRT/ASR_Evaluation/whisper_output'")
        print("   )")