"""
è©å½™è¡¨è½‰æ›å¼•æ“,å°‡master_vocabulary.csvè½‰åŒ–ä¸‰æ ¼å¼
1. google_phrases.json (ç”¨é€”1. Google STT Phraseset)
2. correction_dict.py  (ç”¨é€”2.Pythonå¾Œè™•ç†ä¿®æ­£å­—å…¸)
3. alert_keywords.json (ç”¨é€”3. Elasticsearchå‘Šè­¦é—œéµå­—)
"""

import os
import csv
import json 
from pathlib import Path

class VocabularyGenerator:
    def __init__(self, csv_path='vocabulary/master_vocabulary.csv'):
        """åˆå§‹åŒ–è©å½™è¡¨ç”¢ç”Ÿå™¨"""
        self.project_root = Path(__file__).parent.parent
        self.csv_path = self.project_root / csv_path
        self.output_dir = self.project_root / 'vocabulary'
        
        # ç¢ºä¿è¼¸å‡ºç›®éŒ„å­˜åœ¨
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # å„²å­˜è®€å–çš„è©å½™è³‡æ–™
        self.vocabulary = []
        
    def load_csv(self):
        """è®€å–ä¸»è©å½™è¡¨ CSV"""
        if not self.csv_path.exists():
            raise FileNotFoundError(f"æ‰¾ä¸åˆ°è©å½™è¡¨æª”æ¡ˆ: {self.csv_path}")
        
        print(f"ğŸ“– æ­£åœ¨è®€å–: {self.csv_path}")
        
        with open(self.csv_path, 'r', encoding='utf-8') as f:
            reader = csv.DictReader(f)
            self.vocabulary = list(reader)
        
        print(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.vocabulary)} å€‹è©å½™")
        return self
    
    def generate_google_phrases(self):
        """
        ç”¢ç”Ÿ Google STT çš„ PhraseSet æ ¼å¼
        ç”¨é€”1: è¾¨è­˜å‰å„ªåŒ–ï¼Œæé«˜å°ˆæ¥­è¡“èªè¾¨è­˜æº–ç¢ºç‡
        """
        output_path = self.output_dir / 'google_phrases.json'
        
        # å»ºç«‹ phrases é™£åˆ—
        phrases = []
        
        for item in self.vocabulary:
            phrase_obj = {
                "value": item['term'],
                "boost": float(item['boost_value'])
            }
            phrases.append(phrase_obj)
        
        # åŒ…è£æˆ Google STT V2 API çš„ PhraseSet æ ¼å¼
        phrase_set = {
            "phrases": phrases
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:
            json.dump(phrase_set, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… å·²ç”¢ç”Ÿ: {output_path}")
        print(f"   â†’ ç”¨é€”1: Google STT è¾¨è­˜å‰å„ªåŒ– (å…± {len(phrases)} å€‹è©çµ„)")
        return self
    
    def generate_correction_dict(self):
        """
        ç”¢ç”Ÿ Python å­—å…¸æ ¼å¼çš„ä¿®æ­£è¡¨
        ç”¨é€”2: è¾¨è­˜å¾Œä¿®æ­£åŒéŸ³ç•°å­—
        """
        output_path = self.output_dir / 'correction_dict.py'
        
        # å»ºç«‹ä¿®æ­£å­—å…¸
        corrections = {}
        
        for item in self.vocabulary:
            # åªè™•ç†æœ‰æ¨™è¨»å¸¸è¦‹éŒ¯èª¤çš„è©å½™
            if item['common_error'] and item['common_error'].strip():
                correct_term = item['term']
                errors = item['common_error'].split('|')
                
                for error in errors:
                    error = error.strip()
                    if error:
                        corrections[error] = correct_term
        
        # å¯«å…¥ Python æª”æ¡ˆ
        with open(output_path, 'w', encoding='utf-8') as f:
            f.write('"""\n')
            f.write('è©å½™ä¿®æ­£å­—å…¸ - è‡ªå‹•ç”¢ç”Ÿ\n')
            f.write('ç”¨é€”2: Python è¾¨è­˜å¾Œè™•ç†ï¼Œä¿®æ­£åŒéŸ³ç•°å­—\n')
            f.write('è«‹å‹¿æ‰‹å‹•ç·¨è¼¯æ­¤æª”æ¡ˆï¼Œè«‹ä¿®æ”¹ master_vocabulary.csv å¾Œé‡æ–°ç”¢ç”Ÿ\n')
            f.write('"""\n\n')
            f.write('# åŒéŸ³ç•°å­—ä¿®æ­£è¡¨\n')
            f.write('CORRECTION_DICT = {\n')   # å®šç¾©ä¸€å€‹åç‚ºCorrection_Dictçš„å­—å…¸ï¼Œä¸¦ä¸”æŠŠè³‡æ–™å¯«å…¥
            
            for wrong, correct in sorted(corrections.items()):   
                f.write(f'    "{wrong}": "{correct}",\n')       
                # correctionså­—å…¸ä¸­å­˜å„²æ¯å€‹éŒ¯èª¤å­—ä¸²èˆ‡å°æ‡‰çš„æ­£ç¢ºå­—ä¸²ï¼Œé€™æ®µç¨‹å¼éæ­·correctionå­—å…¸æ™‚ï¼Œå°‡æ¯å€‹éŒ¯èª¤å­—ä¸²çµ¦wrongã€å°æ‡‰çš„æ­£ç¢ºå­—ä¸²çµ¦correct
            
            f.write('}\n\n')
            
            # åŠ å…¥è¼”åŠ©å‡½æ•¸
            f.write('def apply_corrections(text):\n')
            f.write('    """\n')
            f.write('    æ‡‰ç”¨è©å½™ä¿®æ­£\n')
            f.write('    \n')
            f.write('    Args:\n')
            f.write('        text (str): è¾¨è­˜åŸå§‹æ–‡å­—\n')
            f.write('    \n')
            f.write('    Returns:\n')
            f.write('        str: ä¿®æ­£å¾Œçš„æ–‡å­—\n')
            f.write('    """\n')
            f.write('    if not text:\n')
            f.write('        return ""\n')
            f.write('    \n')
            f.write('    # åŸ·è¡Œå­—å…¸æ›¿æ›\n')
            f.write('    for wrong, correct in CORRECTION_DICT.items():\n')
            f.write('        text = text.replace(wrong, correct)\n')
            f.write('    \n')
            f.write('    return text\n')
        
        print(f"âœ… å·²ç”¢ç”Ÿ: {output_path}")
        print(f"   â†’ ç”¨é€”2: Python è¾¨è­˜å¾Œä¿®æ­£ (å…± {len(corrections)} çµ„ä¿®æ­£è¦å‰‡)")
        return self
    
    def generate_alert_keywords(self):
        """
        ç”¢ç”Ÿ Elasticsearch å‘Šè­¦é—œéµå­— JSON
        ç”¨é€”3: é—œéµå­—åµæ¸¬èˆ‡åˆ†ç´šå‘Šè­¦
        """
        output_path = self.output_dir / 'alert_keywords.json'
        
        # ä¾å‘Šè­¦ç­‰ç´šåˆ†é¡
        alert_categories = {
            "emergency": [],      # ç­‰ç´š 3
            "important": [],      # ç­‰ç´š 2
            "normal": [],         # ç­‰ç´š 1
            "info": []           # ç­‰ç´š 0 (ä¸å‘Šè­¦ï¼Œä½†è¨˜éŒ„)
        }
        
        for item in self.vocabulary:
            alert_level = int(item['alert_level'])
            keyword_obj = {
                "term": item['term'],
                "category": item['category'],
                "description": item['description']
            }
            
            if alert_level == 3:
                alert_categories["emergency"].append(keyword_obj)
            elif alert_level == 2:
                alert_categories["important"].append(keyword_obj)
            elif alert_level == 1:
                alert_categories["normal"].append(keyword_obj)
            else:
                alert_categories["info"].append(keyword_obj)
        
        # å»ºç«‹ Elasticsearch Percolate Query æ ¼å¼
        es_queries = []
        
        # ç·Šæ€¥å‘Šè­¦ (ç­‰ç´š 3)
        if alert_categories["emergency"]:
            es_queries.append({
                "query_id": "emergency_alert",
                "alert_level": 3,
                "alert_type": "ç·Šæ€¥å‘Šè­¦",
                "notification": ["email", "sms", "popup", "sound"],
                "keywords": [k["term"] for k in alert_categories["emergency"]],
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"content": k["term"]}} 
                            for k in alert_categories["emergency"]
                        ],
                        "minimum_should_match": 1
                    }
                }
            })
        
        # é‡è¦å‘Šè­¦ (ç­‰ç´š 2)
        if alert_categories["important"]:
            es_queries.append({
                "query_id": "important_alert",
                "alert_level": 2,
                "alert_type": "é‡è¦å‘Šè­¦",
                "notification": ["email", "popup"],
                "keywords": [k["term"] for k in alert_categories["important"]],
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"content": k["term"]}} 
                            for k in alert_categories["important"]
                        ],
                        "minimum_should_match": 1
                    }
                }
            })
        
        # ä¸€èˆ¬é€šçŸ¥ (ç­‰ç´š 1)
        if alert_categories["normal"]:
            es_queries.append({
                "query_id": "normal_alert",
                "alert_level": 1,
                "alert_type": "ä¸€èˆ¬é€šçŸ¥",
                "notification": ["popup"],
                "keywords": [k["term"] for k in alert_categories["normal"]],
                "query": {
                    "bool": {
                        "should": [
                            {"match": {"content": k["term"]}} 
                            for k in alert_categories["normal"]
                        ],
                        "minimum_should_match": 1
                    }
                }
            })
        
        output_data = {
            "description": "é—œéµå­—å‘Šè­¦è¨­å®š - è‡ªå‹•ç”¢ç”Ÿ",
            "categories": alert_categories,
            "elasticsearch_queries": es_queries
        }
        
        with open(output_path, 'w', encoding='utf-8') as f:              # è¬›æŒ‡å®šçš„output_pathæª”æ¡ˆä»¥UTF-8ç·¨ç¢¼é–‹å•Ÿï¼Œä½¿ç”¨withé™³è¿°å¥ï¼Œæœƒåœ¨æª”æ¡ˆåŸ·è¡Œé‹ç®—å¾Œé—œé–‰
            json.dump(output_data, f, ensure_ascii=False, indent=2)    # ä½¿ç”¨json.dumpå°‡output_dataå…§å®¹å¯«å…¥è©²æª”æ¡ˆï¼Œå¯ç¢ºä¿éASCIIå­—å…ƒä¸æœƒè¢«è½‰è­°ï¼Œä¸¦ä¸”å·²2å€‹ç©ºæ ¼çš„ç¸®æ’æ–¹å¼æ’ç‰ˆ
        
        print(f"âœ… å·²ç”¢ç”Ÿ: {output_path}")
        print(f"   â†’ ç”¨é€”3: Elasticsearch å‘Šè­¦ (ç·Šæ€¥:{len(alert_categories['emergency'])} "
              f"é‡è¦:{len(alert_categories['important'])} "
              f"ä¸€èˆ¬:{len(alert_categories['normal'])})")
        return self
    
    def generate_statistics(self):
        """ç”¢ç”Ÿè©å½™çµ±è¨ˆå ±å‘Š"""
        stats_path = self.output_dir / 'vocabulary_stats.txt'
        
        # çµ±è¨ˆå„é¡åˆ¥æ•¸é‡
        category_count = {}
        alert_count = {0: 0, 1: 0, 2: 0, 3: 0}
        
        for item in self.vocabulary:   # self.vocabulary æ˜¯ä¸€å€‹å±¬æ€§ï¼Œå­˜å„²äº†å¾ CSV æª”æ¡ˆè®€å–çš„è©å½™è³‡æ–™ã€‚self.vocabulary å¯ä»¥åœ¨ generate_statisticsæ–¹æ³•ä¸­è¨ªå•å’Œä½¿ç”¨ã€‚
            # çµ±è¨ˆé¡åˆ¥
            cat = item['category']
            category_count[cat] = category_count.get(cat, 0) + 1
            
            # çµ±è¨ˆå‘Šè­¦ç­‰ç´š
            level = int(item['alert_level'])
            alert_count[level] += 1
        
        with open(stats_path, 'w', encoding='utf-8') as f:
            f.write('='*60 + '\n')
            f.write('è©å½™è¡¨çµ±è¨ˆå ±å‘Š\n')
            f.write('='*60 + '\n\n')
            
            f.write(f'ç¸½è©å½™æ•¸: {len(self.vocabulary)}\n\n')
            
            f.write('é¡åˆ¥åˆ†å¸ƒ:\n')
            for cat, count in sorted(category_count.items(), key=lambda x: x[1], reverse=True):
                f.write(f'  â€¢ {cat:20s}: {count:3d} å€‹\n')
            
            f.write('\nå‘Šè­¦ç­‰ç´šåˆ†å¸ƒ:\n')
            f.write(f'  â€¢ ç­‰ç´š 0 (ä¸å‘Šè­¦):  {alert_count[0]:3d} å€‹\n')
            f.write(f'  â€¢ ç­‰ç´š 1 (ä¸€èˆ¬):    {alert_count[1]:3d} å€‹\n')
            f.write(f'  â€¢ ç­‰ç´š 2 (é‡è¦):    {alert_count[2]:3d} å€‹\n')
            f.write(f'  â€¢ ç­‰ç´š 3 (ç·Šæ€¥):    {alert_count[3]:3d} å€‹\n')
            
            f.write('\nç”¢ç”Ÿæª”æ¡ˆ:\n')
            f.write(f'  1. google_phrases.json     (ç”¨é€”1: Google STT è¾¨è­˜å‰å„ªåŒ–)\n')
            f.write(f'  2. correction_dict.py      (ç”¨é€”2: Python è¾¨è­˜å¾Œä¿®æ­£)\n')
            f.write(f'  3. alert_keywords.json     (ç”¨é€”3: Elasticsearch å‘Šè­¦)\n')
            f.write(f'  4. vocabulary_stats.txt    (çµ±è¨ˆå ±å‘Š)\n')
        
        print(f"âœ… å·²ç”¢ç”Ÿ: {stats_path}")
        print(f"\nğŸ“Š è©å½™çµ±è¨ˆ:")
        print(f"   ç¸½è¨ˆ: {len(self.vocabulary)} å€‹è©å½™")
        print(f"   é¡åˆ¥æ•¸: {len(category_count)} ç¨®")
        print(f"   ç·Šæ€¥é—œéµå­—: {alert_count[3]} å€‹")
        
        return self
    
    def run(self):
        """åŸ·è¡Œå®Œæ•´çš„è½‰æ›æµç¨‹"""
        print("\n" + "="*60)
        print("ğŸš€ è©å½™è¡¨è½‰æ›å¼•æ“å•Ÿå‹•")
        print("="*60 + "\n")
        
        try:
            self.load_csv()
            print()
            
            self.generate_google_phrases()
            self.generate_correction_dict()
            self.generate_alert_keywords()
            self.generate_statistics()
            
            print("\n" + "="*60)
            print("âœ¨ å…¨éƒ¨å®Œæˆï¼ä¸‰ç¨®æ ¼å¼çš„è©å½™è¡¨å·²æˆåŠŸç”¢ç”Ÿ")
            print("="*60 + "\n")
            
            print("ğŸ“ ç”¢ç”Ÿçš„æª”æ¡ˆä½æ–¼: vocabulary/")
            print("   â”œâ”€ google_phrases.json      (ç”¨é€”1)")
            print("   â”œâ”€ correction_dict.py       (ç”¨é€”2)")
            print("   â”œâ”€ alert_keywords.json      (ç”¨é€”3)")
            print("   â””â”€ vocabulary_stats.txt     (çµ±è¨ˆå ±å‘Š)")
            
            print("\nğŸ”„ ä¸‹ä¸€æ­¥:")
            print("   1. æª¢æŸ¥ç”¢ç”Ÿçš„æª”æ¡ˆæ˜¯å¦æ­£ç¢º")
            print("   2. åœ¨ batch_inference.py ä¸­å¼•ç”¨ correction_dict")
            print("   3. åœ¨ Google STT è¨­å®šä¸­è¼‰å…¥ google_phrases.json")
            print("   4. åœ¨ Elasticsearch ä¸­è¨­å®š alert_keywords.json")
            
        except FileNotFoundError as e:
            print(f"\nâŒ éŒ¯èª¤: {e}")
            print("   è«‹ç¢ºèª vocabulary/master_vocabulary.csv æª”æ¡ˆå­˜åœ¨")
        except Exception as e:
            print(f"\nâŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
            import traceback
            traceback.print_exc()


def main():
    """ä¸»ç¨‹å¼å…¥å£"""
    generator = VocabularyGenerator()
    generator.run()


if __name__ == "__main__":
    main()